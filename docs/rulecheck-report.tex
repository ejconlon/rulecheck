\documentclass[format=sigconf, nonacm=true, review=true, screen=true]{acmart}

\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage{fancyvrb}
\usepackage{minted}
\usepackage{xspace}
\usepackage{hyperref}

% Use this instead of caption to remove acmart description warnings
\newcommand{\mycaption}[1]{\Description{#1}\caption{#1}}

% Project name
\newcommand{\Rulecheck}{\textit{Rulecheck}\xspace}

% For highlighting some texts
% \newcommand{\red}[1]{\textcolor{red}{#1}}

% This appears to fix some font problem...
\DeclareRobustCommand{\ttfamily}{\fontencoding{T1}\fontfamily{lmtt}\selectfont}

% Junk for acmart:
\setcopyright{acmcopyright}
\copyrightyear{2022}
\acmYear{2022}
\acmDOI{N/A}
\acmBooktitle{N/A}

\title{Rulecheck}
\subtitle{Subtitle here}
\author{Zack Grannan}
\author{Eric Conlon}
\authorsaddresses{}
\date{2022-12-xx}

\begin{document}

\begin{abstract}

In this paper, we introduce \Rulecheck, a system for generating test cases for Haskell rewrite rules.  We cover two approaches to test generation, one using \texttt{QuickCheck}, and the other with tactics-based program synthesis. \Rulecheck tests find many instances in which contextual equivalence is not preserved by rewrite rules in various libraries.

\end{abstract}

\maketitle

\section{Introduction}

Optimizing compilers employ a number of techniques that transform code to yield the same result with better runtime characteristics. These transformations are justified by the semantics of the language under consideration. For most languages the definitions of the transformations are static, defined within the compiler itself. In contrast, the Haskell programming language (as implemented in the Glasgow Haskell Compiler, GHC) allows library authors to implement their own compile-time optimizations in the form of rewrite rules. Conceptually, rewrite rules define syntactic transformations that GHC will apply early in the compilation pipeline. Many common low-level libraries (like \texttt{vector}, \texttt{text}, and \texttt{bytestring}) make heavy use of rewrite rules to eliminate redundant transformations, exposing high-level interfaces with excellent runtime characteristics. \cite{coutts2007stream, chakravarty2002approach, chakravarty2007data, shortcutwiki}

GHC interprets rewrite rules as left-to-right substitutions, making several passes over the program as new points of application appear. The rules themselves are type-checked, but otherwise the compiler makes no effort to ensure that they are sensible replacements. The GHC User's Guide highlights the danger: ``GHC makes absolutely no attempt to verify that the LHS and RHS of a rule have the same meaning. That is undecidable in general, and infeasible in most interesting cases. The responsibility is entirely the programmer's!" \cite{userguide} As implemented, rewrite rules are a high-risk but high-reward language feature. Incorrect statements can result in bugs that are difficult to understand due to the early and automatic application of rewrites, as well as the interaction between rules.

We would like library authors (ourselves included) to have more confidence in the correctness of their rewrite rules, and we would like them to be able to accurately identify the causes of rewrite-induced bugs. However, formal proofs of equivalence may be difficult or impossible to construct, and moreover they would require the integration of a theorem prover. We believe that test case generation is a simple alternative that can give users confidence that their rewrites preserve the meaning of their programs.

\section{Methodology}

Generating tests for rewrite rules comes with its own complications. First, rewritten expressions are intended to be \textit{semantically equivalent} to their source expressions, and what that means varies from library to library. For some, this is true contextual equivalence, and for others this is numerical approximation. To make matters worse, a rewrite may not even preserve user-defined boolean equality (from \texttt{class Eq}) - if that is even defined for the type! So in short, we need to remain flexible in the degree and manner in which we assert equality.

The flexibility of the rewrite system flexibility is another challenge. One of the few syntactic restrictions of rewrite rules is that ``the left hand side of a rule must consist of a top-level variable applied to arbitrary expressions'' \cite{userguide} - in so many words, rewrite rules can match arbitrary function applications, and transform terms of arbitrary types. These types are not required to implement any particular interfaces, nor are they even required to represent data! (They can be function-typed and are eta-expanded as needed.) Therefore, we need to be prepared to generate terms of a variety of types to consume the results of applying rewrites.

Finally, though parametricity greatly broadens the applicability of rules, it presents difficulties in test generation. We must choose monotypes to instantiate rule polytypes, taking care that our choices implement any required typeclass constraints.

In summary, this is our simple methodology for testing rules:

\begin{itemize}
  \item Given a type, generate an arbitrary values of (instantiations of) that type
  \item Given two arbitrary values of the same type, compare them for semantic equality
\end{itemize}

\subsection{Generating arbitrary values}

Stuff here...

\subsection{Determining semantic equality}

Stuff here...

\section{Implementation}

\Rulecheck consists of several components:

\begin{itemize}
  \item Tooling to identify and download public Haskell packages
  \item A program to extract rules and types from packages and generate tests
  \item A small runtime library for those generate tests
  \item A library for program synthesis
\end{itemize}

\subsection{Program synthesis}

\Rulecheck implements a tactics-based search for terms matching a given type, with fair interleaving and backtracking. \cite{delahaye2000tactic, kiselyov2005backtracking} It can be configured to read interface files in a format similar to Hoogle+ \cite{james2020digging} to define the sets of types and terms used in the search. Given a goal type, \Rulecheck derives terms of that type by repeatedly applying these tactics:

\begin{itemize}
  \item Yield a variable in the context (i.e. a function argument)
  \item Yield a literal (e.g. Int, String, etc)
  \item Yield a known term (e.g. functions or constants matching exactly)
  \item Apply a function and finding arguments
  \item Apply a function to a variable in the context
  \item Introduce a variable and finding a body (for function goals)
  \item Pattern match on an argument (for function goals with known argument constructors)
\end{itemize}

\Rulecheck directs the search by incrementally performing first-order unification on types. It also eagerly substitutes types implementing typeclass constraints, it is important that all relevant instance declarations be included in the interface file. This process yields terms and their monotypes until terminated or until the search space is exhausted.

The clearest limitation of \Rulecheck is that it does not support \textit{higher-rank} types (those with additional \texttt{forall} quantifiers to the left of a function arrow, not just on the outside of the type). One can see this feature used for encapsulation in the \texttt{ST} and \texttt{Cont} monads, or in the core definitions of lens libraries. We leave synthesis with these types for future work.

\section{Results}

We used the Stackage package repository resolver \texttt{lts-19.27} for GHC \texttt{9.0.2} to identify X packages with rewrite rules. \cite{stackage} This represents Y percent of the total packages available in the package set. (The package set itself contains versions of the most popular Haskell libraries and many more in the long tail that are all mutually compatible.)

Stuff here...

\section{Conclusion}

Conclusion...

\bibliographystyle{ACM-Reference-Format}
\bibliography{rulecheck-report}

\end{document}
